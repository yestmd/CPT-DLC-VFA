{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## To combine the VFA orientation from 1st and 2nd cohort data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob as glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## cohort 1st vfa tot baseline results and info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### handle male mice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#editing abet ii raw data for syncing:change file name and add feature in file\n",
    "abetraw=\"the directory of folder of the male raw ABETII data of each session\" # change here tobe full folder directory in the double quotes\n",
    "edit_abetraw=abetraw + \"\\\\edit_raw_file\"\n",
    "\n",
    "files=[fileName for fileName in os.listdir(abetraw) if fileName.endswith(\".csv\")]\n",
    "\n",
    "if os.path.isdir(edit_abetraw) == False:\n",
    "    os.mkdir(edit_abetraw)\n",
    "    \n",
    "os.chdir(abetraw)\n",
    "\n",
    "for name in files:\n",
    "    df=pd.read_csv(name)\n",
    "    #df.loc[:, \"Schedule run date short\"]=pd.to_datetime(df[\"Date/Time\"]).dt.date\n",
    "    df[\"Schedule run date short\"]=pd.to_datetime(df[\"Date/Time\"], format=\"%m/%d/%Y %I:%M:%S %p\")\n",
    "    df[\"Schedule run date short\"]=df[\"Schedule run date short\"].apply(lambda x: datetime.strftime(x, '%m-%d-%Y'))\n",
    "    df[\"file_name\"]=df[\"Schedule run date short\"] + \"-\" + df[\"Group ID\"] + \"-\" + df[\"Animal ID\"]\n",
    "    \n",
    "    df=df.drop(['Machine Name', 'Date/Time', 'Version', 'Version Name', 'Application_Version', \n",
    "                'Experiment', 'Max_Number_Trials', 'Schedule_Description', 'Environment', \n",
    "                'Analysis Name', 'Schedule Run ID', 'Start ITI - Start'], axis=1)\n",
    "    #drop \"Start ITI - Start\" column, since it impact the orientation determination in next step\n",
    "    df=df.melt(id_vars=['Database','Schedule Name', 'Schedule_Start_Time', 'Schedule run date short', 'Animal ID', 'Group ID', 'file_name', 'Max_Schedule_Time'], var_name=\"event\", value_name=\"sec_cal\")\n",
    "        \n",
    "    n=df[\"file_name\"][1]\n",
    "    df.to_csv(edit_abetraw + \"\\\\%s.csv\"%n)\n",
    "    print(df[\"file_name\"][1] + \":Files's names are updated\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load file name code and started frame info\n",
    "coh1_male_code=pd.read_csv(\"...\\\\file_frame_info_male.csv\", index_col=False) #replace the \"...\" to a full directory "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #update code file\n",
    "# # coh1_male_code=coh1_male_code.rename(columns={'fps':'n_fps', 'fm':'frame'})\n",
    "# coh1_male_code.loc[:,'fm_started']=(coh1_male_code['min']*60 + coh1_male_code['sec'])*coh1_male_code['n_fps'] + coh1_male_code['frame']\n",
    "# # coh1_male_code[\"abet_raw\"]=coh1_male_code[\"name_2\"] + '.csv'\n",
    "# # coh1_male_code[\"abet_raw\"]=coh1_male_code[\"abet_raw\"].apply(lambda x: x.replace('-TOT', ''))\n",
    "# #coh1_male_code['file_name']=coh1_male_code[\"name_2\"]+\".csv\"\n",
    "# coh1_male_code.to_csv(r\"D:\\\\CPT_TOT_DS_Stage_3\\\\CPT_DS_TOT_Visual_analysis\\\\file_frame_info_male.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#add feature for vfa result, and create new vfa result into temp folder\n",
    "originvfa=\"The full folder directory to vfa result folder\" # replace with the full folder directory in the double quotes\n",
    "\n",
    "temp=originvfa+ \"\\\\temp\"\n",
    "\n",
    "# change frames_cal column into each vfa results file \n",
    "\n",
    "#add temp folder into target path\n",
    "if os.path.isdir(temp) == False:\n",
    "    os.mkdir(temp)\n",
    "    \n",
    "#files=[fileName for fileName in os.listdir(origin) if fileName.endswith(\".csv\")]\n",
    "\n",
    "os.chdir(originvfa)\n",
    "\n",
    "#add sec_cal and direction feature to each vfa result file    \n",
    "for i, j, h, n in zip(coh1_male_code['new_csv'], coh1_male_code[\"fm_started\"],\n",
    "                      coh1_male_code[\"n_fps\"], coh1_male_code['abet_raw']):\n",
    "    df=pd.read_csv(i)\n",
    "    df['frames_cal'] = df['index']-j+1 \n",
    "    df['sec_cal']=(df[\"frames_cal\"]-1)*1/h+0.001\n",
    "    df['n_fps']=h\n",
    "    df[\"total_visual\"]=df[\"lateralRightright\"] + df[\"lateralLeftright\"] + df[\"frontalright\"]\n",
    "    df['blind_or_not']=np.where(df['blindright']>=0.9, 'blind', 'oriented')\n",
    "    \n",
    "    #save editted vfa result file into temp with recovered name\n",
    "    df.to_csv(temp+ \"\\\\%s\"%(n), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ True])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check file name match\n",
    "raw_list=glob.glob(edit_abetraw + \"\\\\*.csv\")\n",
    "raw_list.sort()\n",
    "\n",
    "os.chdir(temp)\n",
    "vfa_list=glob.glob(r\"*.csv\")\n",
    "vfa_list.sort()\n",
    "\n",
    "d={'raw_list':raw_list, 'vfa_list': vfa_list}\n",
    "file_list=pd.DataFrame(data=d)\n",
    "file_list['match']=file_list.apply(lambda x : x.vfa_list[:-4] in x.raw_list, axis=1)\n",
    "file_list['match'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\anaconda\\lib\\site-packages\\ipykernel_launcher.py:33: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "D:\\anaconda\\lib\\site-packages\\ipykernel_launcher.py:34: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "D:\\anaconda\\lib\\site-packages\\ipykernel_launcher.py:48: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "D:\\anaconda\\lib\\site-packages\\ipykernel_launcher.py:49: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "D:\\anaconda\\lib\\site-packages\\ipykernel_launcher.py:63: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "D:\\anaconda\\lib\\site-packages\\ipykernel_launcher.py:64: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    }
   ],
   "source": [
    "#sync abet raw data to vfa result\n",
    "# merge cpt raw data and vfa data to sync event\n",
    "#save processed file into the pathway below:\n",
    "\n",
    "pathfile=originvfa+ \"\\\\temp_2\"\n",
    "if os.path.isdir(pathfile) == False:\n",
    "    os.mkdir(pathfile)\n",
    "\n",
    "\n",
    "os.chdir(temp)\n",
    "\n",
    "for i, j in zip(raw_list, vfa_list):\n",
    "    df=pd.read_csv(i, index_col=None, header=0)\n",
    "    evnt=df[[\"event\", \"sec_cal\"]].copy()\n",
    "    evnt.dropna(subset=['sec_cal'], inplace=True)\n",
    "    evnt[\"count\"]=1\n",
    "    #load vfa data and reform the format of the dataframe    \n",
    "    df=pd.read_csv(j)\n",
    "    \n",
    "    #seperating handling Center Screen Touch - Time event, since it comes with Hit, \n",
    "    #and makes the orientated hit tobe undeterminded\n",
    "   \n",
    "    #merge cpt raw data, not includes center screen touch event and Stim Onset - End ITI, and vfa data\n",
    "    combine_1=pd.concat([evnt[(evnt[\"event\"]!=\"Center Screen Touch - Time\") & (evnt[\"event\"]!=\"Stim Onset - End ITI\")], df], \n",
    "                        ignore_index=False).sort_values(by='sec_cal').reset_index()\n",
    "\n",
    "    #check each row, add ori_before and ori_after\n",
    "    combine_1['ori_before']=\"\"\n",
    "    combine_1[\"ori_after\"]=\"\"\n",
    "\n",
    "    for h in range(len(combine_1)):\n",
    "        if combine_1[\"count\"][h]==1:\n",
    "            combine_1.loc[:, \"ori_before\"][h]=combine_1['blind_or_not'][h-1]\n",
    "            combine_1.loc[:, \"ori_after\"][h]=combine_1['blind_or_not'][h+1]\n",
    "                    \n",
    "    combine_1['ori'] = np.where(combine_1[\"ori_before\"] == combine_1[\"ori_after\"], combine_1['ori_after'], \"undetermined\")\n",
    "    \n",
    "    \n",
    "    #merge cpt raw data only includes center screen touch event and vfa data\n",
    "    combine_2=pd.concat([evnt[evnt[\"event\"]==\"Center Screen Touch - Time\"], df], ignore_index=False).sort_values(by='sec_cal').reset_index()\n",
    "\n",
    "    #check each row, add ori_before and ori_after\n",
    "    combine_2['ori_before']=\"\"\n",
    "    combine_2[\"ori_after\"]=\"\"\n",
    "\n",
    "    for h in range(len(combine_2)):\n",
    "        if combine_2[\"count\"][h]==1:\n",
    "            combine_2.loc[:, \"ori_before\"][h]=combine_2['blind_or_not'][h-1]\n",
    "            combine_2.loc[:, \"ori_after\"][h]=combine_2['blind_or_not'][h+1]\n",
    "                    \n",
    "    combine_2['ori'] = np.where(combine_2[\"ori_before\"] == combine_2[\"ori_after\"], combine_2['ori_after'], \"undetermined\")\n",
    "\n",
    "    \n",
    "    #merge cpt raw data only Stim Onset - End ITI and vfa data\n",
    "    combine_3=pd.concat([evnt[evnt[\"event\"]==\"Stim Onset - End ITI\"], df], ignore_index=False).sort_values(by='sec_cal').reset_index()\n",
    "\n",
    "    #check each row, add ori_before and ori_after\n",
    "    combine_3['ori_before']=\"\"\n",
    "    combine_3[\"ori_after\"]=\"\"\n",
    "\n",
    "    for h in range(len(combine_2)):\n",
    "        if combine_3[\"count\"][h]==1:\n",
    "            combine_3.loc[:, \"ori_before\"][h]=combine_3['blind_or_not'][h-1]\n",
    "            combine_3.loc[:, \"ori_after\"][h]=combine_3['blind_or_not'][h+1]\n",
    "                    \n",
    "    combine_3['ori'] = np.where(combine_3[\"ori_before\"] == combine_3[\"ori_after\"], combine_3['ori_after'], \"undetermined\")   \n",
    "    \n",
    "    #combine the two part results\n",
    "    combine=pd.concat([combine_1, combine_2[combine_2[\"event\"]==\"Center Screen Touch - Time\"], \n",
    "                       combine_3[combine_3[\"event\"]==\"Stim Onset - End ITI\"]], ignore_index=False).sort_values(by='sec_cal')\n",
    "    \n",
    "    \n",
    "    combine=combine.drop(['level_0'], axis=1)\n",
    "    \n",
    "    #add 45min bin and 15min bin feature\n",
    "\n",
    "    conditions_15=[(combine[\"sec_cal\"]<0.000),\n",
    "            (combine[\"sec_cal\"]>=0.000) & (combine[\"sec_cal\"]<=900.000),\n",
    "            (combine[\"sec_cal\"]>900.000) & (combine[\"sec_cal\"]<=1800.000), \n",
    "            (combine[\"sec_cal\"]>1800.000) & (combine[\"sec_cal\"]<=2700.000),\n",
    "            (combine[\"sec_cal\"]>2700.000) & (combine[\"sec_cal\"]<=3600.000),\n",
    "            (combine[\"sec_cal\"]>3600.000) & (combine[\"sec_cal\"]<=4500.000),\n",
    "            (combine[\"sec_cal\"]>4500.000) & (combine[\"sec_cal\"]<=5400.000),\n",
    "                  (combine[\"sec_cal\"]>5400.000)]\n",
    "\n",
    "    values_15=['before_session','15bin_1', '15bin_2', '15bin_3', '15bin_4', '15bin_5', '15bin_6', 'after_session'] \n",
    "\n",
    "    conditions_45=[(combine[\"sec_cal\"]<0.000),\n",
    "                   (combine[\"sec_cal\"]>=0.000) & (combine[\"sec_cal\"]<=2700.000),\n",
    "            (combine[\"sec_cal\"]>2700.000) & (combine[\"sec_cal\"]<=5400.000), (combine[\"sec_cal\"]>5400.000)]\n",
    "\n",
    "    values_45=['before_session',\"first_45min\", \"last_45min\", \"after_session\"] \n",
    "    \n",
    "       \n",
    "    combine['45min_bin']=np.select(conditions_45, values_45)\n",
    "    \n",
    "    combine['15min_bin']=np.select(conditions_15, values_15)\n",
    "    \n",
    "    combine=combine.round(3)\n",
    "    combine['cohort']='coh1'\n",
    "    \n",
    "    combine.to_csv(pathfile + \"\\\\%s\"%j)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### handle female mice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#editing abet ii raw data for syncing:change file name and add feature in file\n",
    "abetraw=\"the directory of folder of the female raw ABETII data of each session\" # change here to be full folder directory in the double quotes \n",
    "\n",
    "edit_abetraw=abetraw + \"edit_raw_file\\\\\"\n",
    "\n",
    "files=[fileName for fileName in os.listdir(abetraw) if fileName.endswith(\".csv\")]\n",
    "\n",
    "os.chdir(abetraw)\n",
    "for name in files:\n",
    "    if os.path.isdir(edit_abetraw) == False:\n",
    "        os.mkdir(edit_abetraw)\n",
    "\n",
    "    df=pd.read_csv(name)\n",
    "    #df.loc[:, \"Schedule run date short\"]=pd.to_datetime(df[\"Date/Time\"]).dt.date\n",
    "    df[\"Schedule run date short\"]=pd.to_datetime(df[\"Date/Time\"], format=\"%m/%d/%Y %I:%M:%S %p\")\n",
    "    df[\"Schedule run date short\"]=df[\"Schedule run date short\"].apply(lambda x: datetime.strftime(x, '%m-%d-%Y'))\n",
    "    df[\"file_name\"]=df[\"Schedule run date short\"] + \"-\" + df[\"Group ID\"] + \"-\" + df[\"Animal ID\"]\n",
    "    \n",
    "    df=df.drop(['Machine Name', 'Date/Time', 'Version', 'Version Name', 'Application_Version', \n",
    "                'Experiment', 'Max_Number_Trials', 'Schedule_Description', 'Environment', \n",
    "                'Analysis Name', 'Schedule Run ID', 'Start ITI - Start'], axis=1)\n",
    "    \n",
    "    df=df.melt(id_vars=['Database','Schedule Name', 'Schedule_Start_Time', 'Schedule run date short', 'Animal ID', 'Group ID', 'file_name', 'Max_Schedule_Time'], var_name=\"event\", value_name=\"sec_cal\")\n",
    "    \n",
    "    n=df[\"file_name\"][1]\n",
    "    df.to_csv(edit_abetraw + \"%s.csv\"%n)\n",
    "    print(df[\"file_name\"][1] + \":Files's names are updated\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name_2</th>\n",
       "      <th>file_name</th>\n",
       "      <th>h</th>\n",
       "      <th>min</th>\n",
       "      <th>sec</th>\n",
       "      <th>frame</th>\n",
       "      <th>n_fps</th>\n",
       "      <th>dlc</th>\n",
       "      <th>dlc_firstchange</th>\n",
       "      <th>new_dlc</th>\n",
       "      <th>mp4</th>\n",
       "      <th>new_mp4</th>\n",
       "      <th>new_csv</th>\n",
       "      <th>match</th>\n",
       "      <th>fm_started</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>12-07-21-A-L-2-TOT-1st.mp4</td>\n",
       "      <td>12-07-2021-A-L-2-TOT-1st.csv</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>25</td>\n",
       "      <td>30</td>\n",
       "      <td>12-07-21-A-L-2-TOT-1stDLC_resnet50_CPT_DS_TOT_...</td>\n",
       "      <td>12-07-21-A-L-2-TOT-1st.csv</td>\n",
       "      <td>mice1_dlc.csv</td>\n",
       "      <td>12-07-21-A-L-2-TOT-1st.mp4</td>\n",
       "      <td>mice1.mp4</td>\n",
       "      <td>mice1.csv</td>\n",
       "      <td>True</td>\n",
       "      <td>1229</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>12-07-21-A-N-4-TOT-1st.mp4</td>\n",
       "      <td>12-07-2021-A-N-4-TOT-1st.csv</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>41</td>\n",
       "      <td>3</td>\n",
       "      <td>30</td>\n",
       "      <td>12-07-21-A-N-4-TOT-1stDLC_resnet50_CPT_DS_TOT_...</td>\n",
       "      <td>12-07-21-A-N-4-TOT-1st.csv</td>\n",
       "      <td>mice2_dlc.csv</td>\n",
       "      <td>12-07-21-A-N-4-TOT-1st.mp4</td>\n",
       "      <td>mice2.mp4</td>\n",
       "      <td>mice2.csv</td>\n",
       "      <td>True</td>\n",
       "      <td>1233</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       name_2                     file_name  h  min  sec  \\\n",
       "0  12-07-21-A-L-2-TOT-1st.mp4  12-07-2021-A-L-2-TOT-1st.csv  0    0   40   \n",
       "1  12-07-21-A-N-4-TOT-1st.mp4  12-07-2021-A-N-4-TOT-1st.csv  0    0   41   \n",
       "\n",
       "   frame  n_fps                                                dlc  \\\n",
       "0     25     30  12-07-21-A-L-2-TOT-1stDLC_resnet50_CPT_DS_TOT_...   \n",
       "1      3     30  12-07-21-A-N-4-TOT-1stDLC_resnet50_CPT_DS_TOT_...   \n",
       "\n",
       "              dlc_firstchange        new_dlc                         mp4  \\\n",
       "0  12-07-21-A-L-2-TOT-1st.csv  mice1_dlc.csv  12-07-21-A-L-2-TOT-1st.mp4   \n",
       "1  12-07-21-A-N-4-TOT-1st.csv  mice2_dlc.csv  12-07-21-A-N-4-TOT-1st.mp4   \n",
       "\n",
       "     new_mp4    new_csv  match  fm_started  \n",
       "0  mice1.mp4  mice1.csv   True        1229  \n",
       "1  mice2.mp4  mice2.csv   True        1233  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#load female file frame info\n",
    "coh1_female_code=pd.read_csv(\"...\\\\file_frame_info_female.csv\", index_col=False) #replace the \"...\" to a full directory \n",
    "coh1_female_code.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#update code file\n",
    "# coh1_female_code=coh1_female_code.rename(columns={'fps':'n_fps', 'fm':'frame'})\n",
    "coh1_female_code.loc[:,'fm_started']=(coh1_female_code['min']*60 + coh1_female_code['sec'])*coh1_female_code['n_fps'] + coh1_female_code['frame']\n",
    "# # coh1_female_code[\"name_2\"]=coh1_female_code[\"name_2\"].str.replace('-tot', '')\n",
    "# # coh1_female_code['file_name']=coh1_female_code[\"name_2\"]+\".csv\"\n",
    "# coh1_female_code.head(2)\n",
    "coh1_female_code.to_csv(\"...\\\\file_frame_info_female.csv\", index=False) #replace the \"...\" to a full directory "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#change vfa result's name and add feature for vfa result\n",
    "originvfa=\"The full folder directory to original vfa result folder\" \n",
    "# change here to be full folder directory in the double quotes \n",
    "\n",
    "# change frames_cal column into each vfa results file \n",
    "#add temp folder into target path\n",
    "pathfile=originvfa+\"\\\\temp\"\n",
    "if os.path.isdir(pathfile) == False:\n",
    "    os.mkdir(pathfile)\n",
    "    \n",
    "os.chdir(originvfa)\n",
    "\n",
    "#add sec_cal and direction feature to each vfa result file    \n",
    "for i, j, h, n in zip(coh1_female_code['new_csv'], coh1_female_code[\"fm_started\"], \n",
    "                      coh1_female_code[\"n_fps\"], coh1_female_code['file_name']):\n",
    "    df=pd.read_csv(i)\n",
    "    df['frames_cal'] = df['index']-j+1 \n",
    "    df['sec_cal']=(df[\"frames_cal\"]-1)*1/h+0.001\n",
    "    df['n_fps']=h\n",
    "    df[\"total_visual\"]=df[\"lateralRightright\"] + df[\"lateralLeftright\"] + df[\"frontalright\"]\n",
    "    df['blind_or_not']=np.where(df['blindright']>=0.9, 'blind', 'oriented')\n",
    "    \n",
    "    df.to_csv(pathfile+ \"\\\\%s\"%(n), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ True])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check file name match\n",
    "raw_list=glob.glob(edit_abetraw + \"\\\\*.csv\")\n",
    "raw_list.sort()\n",
    "\n",
    "os.chdir(originvfa+\"\\\\temp\\\\\")\n",
    "vfa_list=glob.glob(r\"*.csv\")\n",
    "vfa_list.sort()\n",
    "\n",
    "d={'raw_list':raw_list, 'vfa_list': vfa_list}\n",
    "file_list=pd.DataFrame(data=d)\n",
    "file_list['match']=file_list.apply(lambda x : x.vfa_list[:-12] in x.raw_list, axis=1)\n",
    "file_list['match'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sync abet raw data to vfa result\n",
    "# merge cpt raw data and vfa data to sync event\n",
    "from scipy.stats import norm\n",
    "import math\n",
    "\n",
    "pathfile=originvfa+\"\\\\temp_2\\\\\"\n",
    "if os.path.isdir(pathfile) == False:\n",
    "    os.mkdir(pathfile)\n",
    "\n",
    "for i, j in zip(raw_list, vfa_list):\n",
    "    df=pd.read_csv(i, index_col=None, header=0)\n",
    "    evnt=df[[\"event\", \"sec_cal\"]].copy()\n",
    "    evnt.dropna(subset=['sec_cal'], inplace=True)\n",
    "    evnt[\"count\"]=1\n",
    "    #load vfa data and reform the format of the dataframe    \n",
    "    df=pd.read_csv(j)\n",
    "    #seperating handling Center Screen Touch - Time event, since it comes with Hit, \n",
    "    #and makes the orientated hit tobe undeterminded\n",
    "   \n",
    "    #merge cpt raw data, not includes center screen touch event and Stim Onset - End ITI, and vfa data\n",
    "    combine_1=pd.concat([evnt[(evnt[\"event\"]!=\"Center Screen Touch - Time\") & (evnt[\"event\"]!=\"Stim Onset - End ITI\")], df], \n",
    "                        ignore_index=False).sort_values(by='sec_cal').reset_index()\n",
    "\n",
    "    #check each row, add ori_before and ori_after\n",
    "    combine_1['ori_before']=\"\"\n",
    "    combine_1[\"ori_after\"]=\"\"\n",
    "\n",
    "    for h in range(len(combine_1)):\n",
    "        if combine_1.loc[h, \"count\"]==1:\n",
    "            combine_1.loc[h, \"ori_before\"]=combine_1.loc[h-1,'blind_or_not']\n",
    "            combine_1.loc[h, \"ori_after\"]=combine_1.loc[h+1, 'blind_or_not']\n",
    "                    \n",
    "    combine_1['ori'] = np.where(combine_1[\"ori_before\"] == combine_1[\"ori_after\"], combine_1['ori_after'], \"undetermined\")\n",
    "    \n",
    "    \n",
    "    #merge cpt raw data only includes center screen touch event and vfa data\n",
    "    combine_2=pd.concat([evnt[evnt[\"event\"]==\"Center Screen Touch - Time\"], df], ignore_index=False).sort_values(by='sec_cal').reset_index()\n",
    "\n",
    "    #check each row, add ori_before and ori_after\n",
    "    combine_2['ori_before']=\"\"\n",
    "    combine_2[\"ori_after\"]=\"\"\n",
    "\n",
    "    for h in range(len(combine_2)):\n",
    "        if combine_2.loc[h, \"count\"]==1:\n",
    "            combine_2.loc[h, \"ori_before\"]=combine_2.loc[h-1,'blind_or_not']\n",
    "            combine_2.loc[h, \"ori_after\"]=combine_2.loc[h+1, 'blind_or_not']\n",
    "                    \n",
    "    combine_2['ori'] = np.where(combine_2[\"ori_before\"] == combine_2[\"ori_after\"], combine_2['ori_after'], \"undetermined\")\n",
    "\n",
    "    \n",
    "    #merge cpt raw data only Stim Onset - End ITI and vfa data\n",
    "    combine_3=pd.concat([evnt[evnt[\"event\"]==\"Stim Onset - End ITI\"], df], ignore_index=False).sort_values(by='sec_cal').reset_index()\n",
    "\n",
    "    #check each row, add ori_before and ori_after\n",
    "    combine_3['ori_before']=\"\"\n",
    "    combine_3[\"ori_after\"]=\"\"\n",
    "\n",
    "    for h in range(len(combine_3)):\n",
    "        if combine_3.loc[h, \"count\"]==1:\n",
    "            combine_3.loc[h, \"ori_before\"]=combine_3.loc[h-1,'blind_or_not']\n",
    "            combine_3.loc[h, \"ori_after\"]=combine_3.loc[h+1, 'blind_or_not']\n",
    "                    \n",
    "    combine_3['ori'] = np.where(combine_3[\"ori_before\"] == combine_3[\"ori_after\"], combine_3['ori_after'], \"undetermined\")   \n",
    "    \n",
    "    #combine the two part results\n",
    "    combine=pd.concat([combine_1, combine_2[combine_2[\"event\"]==\"Center Screen Touch - Time\"], \n",
    "                       combine_3[combine_3[\"event\"]==\"Stim Onset - End ITI\"]], ignore_index=False).sort_values(by='sec_cal')\n",
    "    \n",
    "    #combine[(combine['ori_before'] != combine['ori_after']), ['ori']] = \"undetermined\"\n",
    "    combine=combine.drop(['level_0'], axis=1)\n",
    "    \n",
    "    #add 45min bin and 15min bin feature\n",
    "\n",
    "    conditions_15=[(combine[\"sec_cal\"]<0.000),\n",
    "            (combine[\"sec_cal\"]>=0.000) & (combine[\"sec_cal\"]<=900.000),\n",
    "            (combine[\"sec_cal\"]>900.000) & (combine[\"sec_cal\"]<=1800.000), \n",
    "            (combine[\"sec_cal\"]>1800.000) & (combine[\"sec_cal\"]<=2700.000),\n",
    "            (combine[\"sec_cal\"]>2700.000) & (combine[\"sec_cal\"]<=3600.000),\n",
    "            (combine[\"sec_cal\"]>3600.000) & (combine[\"sec_cal\"]<=4500.000),\n",
    "            (combine[\"sec_cal\"]>4500.000) & (combine[\"sec_cal\"]<=5400.000),\n",
    "                  (combine[\"sec_cal\"]>5400.000)]\n",
    "\n",
    "    values_15=['before_session','15bin_1', '15bin_2', '15bin_3', '15bin_4', '15bin_5', '15bin_6', 'after_session'] \n",
    "\n",
    "    conditions_45=[(combine[\"sec_cal\"]<0.000),\n",
    "                   (combine[\"sec_cal\"]>=0.000) & (combine[\"sec_cal\"]<=2700.000),\n",
    "            (combine[\"sec_cal\"]>2700.000) & (combine[\"sec_cal\"]<=5400.000), (combine[\"sec_cal\"]>5400.000)]\n",
    "\n",
    "    values_45=['before_session',\"first_45min\", \"last_45min\", \"after_session\"] \n",
    "    \n",
    "       \n",
    "    combine['45min_bin']=np.select(conditions_45, values_45)\n",
    "    \n",
    "    combine['15min_bin']=np.select(conditions_15, values_15)\n",
    "    \n",
    "    combine=combine.round(3)\n",
    "    combine['cohort']='coh1'\n",
    "    \n",
    "    combine.to_csv(originvfa+\"\\\\temp_2\\\\%s\"%j)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# get 2nd cohort vfa tot baseline results and info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### recover file name in result folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#editing abet ii raw data for syncing:change file name and add feature in file\n",
    "abetraw=\"the directory of folder of the cohort 2 raw ABETII data of each session\" \n",
    "# change here tobe full folder directory in the double quotes\n",
    "\n",
    "eidt_abetraw=abetraw + \"\\\\edit_raw_file\"\n",
    "\n",
    "files=[fileName for fileName in os.listdir(abetraw) if fileName.endswith(\".csv\")]\n",
    "\n",
    "os.chdir(abetraw)\n",
    "for name in files:\n",
    "    if os.path.isdir(edit_abetraw) == False:\n",
    "        os.mkdir(edit_abetraw)\n",
    "\n",
    "    df=pd.read_csv(name)\n",
    "    #df.loc[:, \"Schedule run date short\"]=pd.to_datetime(df[\"Date/Time\"]).dt.date\n",
    "    df[\"Schedule run date short\"]=pd.to_datetime(df[\"Date/Time\"], format=\"%m/%d/%Y %I:%M:%S %p\")\n",
    "    df[\"Schedule run date short\"]=df[\"Schedule run date short\"].apply(lambda x: datetime.strftime(x, '%m-%d-%Y'))\n",
    "    df[\"file_name\"]=df[\"Schedule run date short\"] + \"-\" + df[\"Group ID\"] + \"-\" + df[\"Animal ID\"]\n",
    "    \n",
    "    df=df.drop(['Machine Name', 'Date/Time', 'Version', 'Version Name', 'Application_Version', \n",
    "                'Experiment', 'Max_Number_Trials', 'Schedule_Description', 'Environment', \n",
    "                'Analysis Name', 'Schedule Run ID', 'Start ITI - Start'], axis=1)\n",
    "    \n",
    "    df=df.melt(id_vars=['Database','Schedule Name', 'Schedule_Start_Time', 'Schedule run date short', 'Animal ID', 'Group ID', 'file_name', 'Max_Schedule_Time'], var_name=\"event\", value_name=\"sec_cal\")\n",
    "        \n",
    "    n=df[\"file_name\"][1]\n",
    "    df.to_csv(edit_abetraw + \"\\\\%s.csv\"%n)\n",
    "    print(df[\"file_name\"][1] + \":Files's names are updated\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #for update name_code\n",
    "# name_code=pd.read_csv(\"...\\\\name_code.csv\") #replace the \"...\" to a full directory \n",
    "# started_frame=pd.read_excel(\"...\\\\started frame_.xlsx\",sheet_name='Sheet2', index_col=None, header=0) #replace the \"...\" to a full directory \n",
    "\n",
    "# coh2_tot_code=started_frame.merge(name_code, on=\"file_name\", how=\"outer\")\n",
    "# coh2_tot_code=coh2_tot_code.rename(columns={'vfa_id':'vfa_csv_1', 'csv_file_name':'vfa_csv_2'})\n",
    "# coh2_tot_code=pd.read_csv(\"...\\\\coh2_tot_code.csv\", index_col=False)\n",
    "# coh2_tot_code.loc[:,'fm_started']=(coh2_tot_code['min']*60 + coh2_tot_code['sec'])*coh2_tot_code['n_fps'] + coh2_tot_code['frame']\n",
    "# coh2_tot_code[\"abet_raw\"]=coh2_tot_code[\"vfa_csv_2\"].apply(lambda x: x.replace('-TOT-1st', ''))\n",
    "# coh2_tot_code[\"abet_raw\"]=coh2_tot_code[\"abet_raw\"].apply(lambda x: x.replace('-TOT-2nd', ''))\n",
    "# coh2_tot_code['file_name']=coh2_tot_code[\"name_2\"]+\".csv\"\n",
    "\n",
    "# coh2_tot_code.to_csv(\"...\\\\coh2_tot_code.csv\") #replace the \"...\" to a full directory "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#add feature for vfa result\n",
    "originvfa=\"The full folder directory to original vfa result folder\" \n",
    "# change here to be full folder directory in the double quotes \n",
    "\n",
    "save_path=originvfa+\"\\\\temp\"\n",
    "# change frames_cal column into each vfa results file \n",
    "\n",
    "#add temp folder into target path\n",
    "if os.path.isdir(save_path) == False:\n",
    "    os.mkdir(save_path)\n",
    "    \n",
    "os.chdir(originvfa)\n",
    "\n",
    "#add sec_cal and direction feature to each vfa result file    \n",
    "for i, j, h, n in zip(coh2_tot_code['vfa_csv_1'], coh2_tot_code[\"fm_started\"], \n",
    "                      coh2_tot_code[\"n_fps\"], coh2_tot_code['vfa_csv_2']):\n",
    "    df=pd.read_csv(i)\n",
    "    df['frames_cal'] = df['index']-j+1 \n",
    "    df['sec_cal']=(df[\"frames_cal\"]-1)*1/h+0.001\n",
    "    df['n_fps']=h\n",
    "    df[\"total_visual\"]=df[\"lateralRightright\"] + df[\"lateralLeftright\"] + df[\"frontalright\"]\n",
    "    df['blind_or_not']=np.where(df['blindright']>=0.9, 'blind', 'oriented')\n",
    "    \n",
    "    df.to_csv(save_path+ \"\\\\%s\"%(n), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ True])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check file name match\n",
    "raw_list=glob.glob(abetraw + \"\\\\*.csv\")\n",
    "raw_list.sort()\n",
    "\n",
    "save_path=originvfa + \"\\\\temp\"\n",
    "os.chdir(save_path)\n",
    "vfa_list=glob.glob(r\"*.csv\")\n",
    "# 04-07-2022-Z-R-16-TOT-2nd.csv was not recorded in ABET \n",
    "#since ABET program error, \n",
    "#remove it from list\n",
    "#vfa_list.remove(\"04-07-2022-Z-R-16-TOT-2nd.csv\")\n",
    "#vfa_list.sort()\n",
    "\n",
    "d={'raw_list':raw_list, 'vfa_list': vfa_list}\n",
    "file_list=pd.DataFrame(data=d)\n",
    "file_list['match']=file_list.apply(lambda x : x.vfa_list[:-12] in x.raw_list, axis=1)\n",
    "file_list['match'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\anaconda\\lib\\site-packages\\ipykernel_launcher.py:34: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "D:\\anaconda\\lib\\site-packages\\ipykernel_launcher.py:35: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "D:\\anaconda\\lib\\site-packages\\ipykernel_launcher.py:49: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "D:\\anaconda\\lib\\site-packages\\ipykernel_launcher.py:50: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "D:\\anaconda\\lib\\site-packages\\ipykernel_launcher.py:64: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "D:\\anaconda\\lib\\site-packages\\ipykernel_launcher.py:65: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    }
   ],
   "source": [
    "#sync abet raw data to vfa result\n",
    "# merge cpt raw data and vfa data to sync event\n",
    "from scipy.stats import norm\n",
    "import math\n",
    "\n",
    "#save the processed file into the path below\n",
    "pathfile=originvfa + \"\\\\temp_2\"\n",
    "if os.path.isdir(pathfile) == False:\n",
    "    os.mkdir(pathfile)\n",
    "\n",
    "save_path=originvfa + \"\\\\temp\"\n",
    "os.chdir(save_path)\n",
    "\n",
    "for i, j in zip(file_list['raw_list'], file_list['vfa_list']):\n",
    "    df=pd.read_csv(i, index_col=None, header=0)\n",
    "    evnt=df[[\"event\", \"sec_cal\"]].copy()\n",
    "    evnt.dropna(subset=['sec_cal'], inplace=True)\n",
    "    evnt[\"count\"]=1\n",
    "    #load vfa data and reform the format of the dataframe    \n",
    "    df=pd.read_csv(j)\n",
    "    #seperating handling Center Screen Touch - Time event, since it comes with Hit, \n",
    "    #and makes the orientated hit tobe undeterminded\n",
    "   \n",
    "    #merge cpt raw data, not includes center screen touch event and Stim Onset - End ITI, and vfa data\n",
    "    combine_1=pd.concat([evnt[(evnt[\"event\"]!=\"Center Screen Touch - Time\") & (evnt[\"event\"]!=\"Stim Onset - End ITI\")], df], \n",
    "                        ignore_index=False).sort_values(by='sec_cal').reset_index()\n",
    "\n",
    "    #check each row, add ori_before and ori_after\n",
    "    combine_1['ori_before']=\"\"\n",
    "    combine_1[\"ori_after\"]=\"\"\n",
    "\n",
    "    for h in range(len(combine_1)):\n",
    "        if combine_1.loc[h, \"count\"]==1:\n",
    "            combine_1.loc[h, \"ori_before\"]=combine_1.loc[h-1,'blind_or_not']\n",
    "            combine_1.loc[h, \"ori_after\"]=combine_1.loc[h+1, 'blind_or_not']\n",
    "                    \n",
    "    combine_1['ori'] = np.where(combine_1[\"ori_before\"] == combine_1[\"ori_after\"], combine_1['ori_after'], \"undetermined\")\n",
    "    \n",
    "    \n",
    "    #merge cpt raw data only includes center screen touch event and vfa data\n",
    "    combine_2=pd.concat([evnt[evnt[\"event\"]==\"Center Screen Touch - Time\"], df], ignore_index=False).sort_values(by='sec_cal').reset_index()\n",
    "\n",
    "    #check each row, add ori_before and ori_after\n",
    "    combine_2['ori_before']=\"\"\n",
    "    combine_2[\"ori_after\"]=\"\"\n",
    "\n",
    "    for h in range(len(combine_2)):\n",
    "        if combine_2.loc[h, \"count\"]==1:\n",
    "            combine_2.loc[h, \"ori_before\"]=combine_2.loc[h-1,'blind_or_not']\n",
    "            combine_2.loc[h, \"ori_after\"]=combine_2.loc[h+1, 'blind_or_not']\n",
    "                    \n",
    "    combine_2['ori'] = np.where(combine_2[\"ori_before\"] == combine_2[\"ori_after\"], combine_2['ori_after'], \"undetermined\")\n",
    "\n",
    "    \n",
    "    #merge cpt raw data only Stim Onset - End ITI and vfa data\n",
    "    combine_3=pd.concat([evnt[evnt[\"event\"]==\"Stim Onset - End ITI\"], df], ignore_index=False).sort_values(by='sec_cal').reset_index()\n",
    "\n",
    "    #check each row, add ori_before and ori_after\n",
    "    combine_3['ori_before']=\"\"\n",
    "    combine_3[\"ori_after\"]=\"\"\n",
    "\n",
    "    for h in range(len(combine_2)):\n",
    "        if combine_3.loc[h, \"count\"]==1:\n",
    "            combine_3.loc[h, \"ori_before\"]=combine_3.loc[h-1,'blind_or_not']\n",
    "            combine_3.loc[h, \"ori_after\"]=combine_3.loc[h+1, 'blind_or_not']\n",
    "                    \n",
    "    combine_3['ori'] = np.where(combine_3[\"ori_before\"] == combine_3[\"ori_after\"], combine_3['ori_after'], \"undetermined\")   \n",
    "    \n",
    "    #combine the two part results\n",
    "    combine=pd.concat([combine_1, combine_2[combine_2[\"event\"]==\"Center Screen Touch - Time\"], \n",
    "                       combine_3[combine_3[\"event\"]==\"Stim Onset - End ITI\"]], ignore_index=False).sort_values(by='sec_cal')\n",
    "    \n",
    "    #combine[(combine['ori_before'] != combine['ori_after']), ['ori']] = \"undetermined\"\n",
    "    combine=combine.drop(['level_0'], axis=1)\n",
    "    \n",
    "    #add 45min bin and 15min bin feature\n",
    "\n",
    "    conditions_15=[(combine[\"sec_cal\"]<0.000),\n",
    "            (combine[\"sec_cal\"]>=0.000) & (combine[\"sec_cal\"]<=900.000),\n",
    "            (combine[\"sec_cal\"]>900.000) & (combine[\"sec_cal\"]<=1800.000), \n",
    "            (combine[\"sec_cal\"]>1800.000) & (combine[\"sec_cal\"]<=2700.000),\n",
    "            (combine[\"sec_cal\"]>2700.000) & (combine[\"sec_cal\"]<=3600.000),\n",
    "            (combine[\"sec_cal\"]>3600.000) & (combine[\"sec_cal\"]<=4500.000),\n",
    "            (combine[\"sec_cal\"]>4500.000) & (combine[\"sec_cal\"]<=5400.000),\n",
    "                  (combine[\"sec_cal\"]>5400.000)]\n",
    "\n",
    "    values_15=['before_session','15bin_1', '15bin_2', '15bin_3', '15bin_4', '15bin_5', '15bin_6', 'after_session'] \n",
    "\n",
    "    conditions_45=[(combine[\"sec_cal\"]<0.000),\n",
    "                   (combine[\"sec_cal\"]>=0.000) & (combine[\"sec_cal\"]<=2700.000),\n",
    "            (combine[\"sec_cal\"]>2700.000) & (combine[\"sec_cal\"]<=5400.000), (combine[\"sec_cal\"]>5400.000)]\n",
    "\n",
    "    values_45=['before_session',\"first_45min\", \"last_45min\", \"after_session\"] \n",
    "    \n",
    "       \n",
    "    combine['45min_bin']=np.select(conditions_45, values_45)\n",
    "    \n",
    "    combine['15min_bin']=np.select(conditions_15, values_15)\n",
    "    \n",
    "    combine=combine.round(3)\n",
    "    combine['cohort']='coh2'\n",
    "    \n",
    "    combine.to_csv(pathfile + \"\\\\%s\"%j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
