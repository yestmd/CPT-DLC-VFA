{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob as glob\n",
    "import pandas as pd\n",
    "import os\n",
    "import shutil\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import norm\n",
    "from scipy.stats import sem\n",
    "# from statsmodels.stats.anova import AnovaRM\n",
    "# import scikit_posthocs as sp\n",
    "# import statsmodels.formula.api as smf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.12.2'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sns.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load started frame info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "disk_path=\"put the disk directory here\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "parent_folder=disk_path + \"\\\\CPT_TOT_DS_Stage_3_2nd\\\\VFA\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "frame_cal=pd.read_excel(parent_folder+\"\\\\started frame_2.xlsx\")\n",
    "# frame_cal['vfa_session']=frame_cal['session name'].str[1:] + \".csv\"\n",
    "# frame_cal['group_id']=frame_cal['group_id'].str[1:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### copy vfa raw file into folder with changed file name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# name_code=pd.read_csv(parent_folder + \"\\\\video and DLC_result name changed\\\\file_list.csv\")\n",
    "# vfa_raw=name_code['vfa_id'].to_list()\n",
    "# vfa_recover=name_code['vfa_session'].to_list()\n",
    "\n",
    "# for i, j in zip (vfa_raw, vfa_recover):\n",
    "#     shutil.copy2(parent_folder + \"\\\\dlc_output\\\\results\\\\%s\"%i, parent_folder + \"\\\\dlc_output\\results\\threshold=3\\%s\"%j)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### add frames_cal column and other features into each vfa results file "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "saveto=parent_folder+\"\\\\dlc_output\\\\results\\\\threshold=3\\\\temp\"\n",
    "\n",
    "if os.path.isdir(saveto) == False:\n",
    "    os.mkdir(saveto)\n",
    "\n",
    "os.chdir(parent_folder+\"\\\\dlc_output\\\\results\\\\threshold=3\")\n",
    "\n",
    "for i, j, in zip(frame_cal['vfa_session'], frame_cal[\"start_frame\"]):\n",
    "    \n",
    "    df=pd.read_csv(i)\n",
    "    df['frames_cal'] = df['index']-j+1 \n",
    "    df['sec_cal']=(df[\"frames_cal\"]-1)*1/60+0.001\n",
    "    \n",
    "    df[\"total_visual\"]=df[\"lateralRightright\"] + df[\"lateralLeftright\"] + df[\"frontalright\"]\n",
    "    df['blind_or_not']=np.where(df['blindright']>=0.9, 'blind', 'oriented')\n",
    "    df=df.round(3)\n",
    "    \n",
    "    df.to_csv(saveto+ \"\\\\%s\"%(i), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### change name of abet raw file and add feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#editing abet ii raw data for syncing:change file name and add feature in file\n",
    "# origin=parent_folder+ \"\\\\abet raw betch\" \n",
    "\n",
    "# target=origin + \"\\\\edit_raw_file\"\n",
    "\n",
    "# files=[fileName for fileName in os.listdir(origin) if fileName.endswith(\".csv\")]\n",
    "\n",
    "# os.chdir(origin)\n",
    "# for name in files:\n",
    "#     if os.path.isdir(target) == False:\n",
    "#         os.mkdir(target)\n",
    "\n",
    "#     df=pd.read_csv(name)\n",
    "#     #df.loc[:, \"Schedule run date short\"]=pd.to_datetime(df[\"Date/Time\"]).dt.date\n",
    "#     df[\"Schedule run date short\"]=pd.to_datetime(df[\"Date/Time\"], format=\"%m/%d/%Y %I:%M:%S %p\")\n",
    "#     df[\"Schedule run date short\"]=df[\"Schedule run date short\"].apply(lambda x: datetime.strftime(x, '%m-%d-%Y'))\n",
    "#     df[\"file_name\"]=df[\"Schedule run date short\"] + \"-\" + df[\"Group ID\"] + \"-\" + df[\"Animal ID\"]\n",
    "    \n",
    "#     df=df.drop(['Machine Name', 'Date/Time', 'Version', 'Version Name', 'Application_Version', \n",
    "#                 'Experiment', 'Max_Number_Trials', 'Schedule_Description', 'Environment', \n",
    "#                 'Analysis Name', 'Schedule Run ID', 'Start ITI - Start'], axis=1)\n",
    "#     #drop \"Start ITI - Start\" column, since it impact the orientation determination in next step\n",
    "#     df=df.melt(id_vars=['Database','Schedule Name', 'Schedule_Start_Time', 'Schedule run date short', 'Animal ID', 'Group ID', 'file_name', 'Max_Schedule_Time'], var_name=\"event\", value_name=\"sec_cal\")\n",
    "        \n",
    "#     n=df[\"file_name\"][1]\n",
    "#     df.to_csv(target + \"\\\\%s.csv\"%n)\n",
    "#     print(df[\"file_name\"][1] + \":Files's names are updated\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### sync abet raw and vfa results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "83\n",
      "83\n"
     ]
    }
   ],
   "source": [
    "raw_list_=glob.glob(disk_path+\"\\\\CPT_TOT_DS_Stage_3_2nd\\\\VFA\\\\abet raw betch\\\\edit_raw_file\\\\*.csv\")\n",
    "raw_list_.sort()\n",
    "\n",
    "target=disk_path+\"\\\\CPT_TOT_DS_Stage_3_2nd\\\\VFA\\\\dlc_output\\\\results\\\\threshold=3\\\\temp\"\n",
    "os.chdir(target)\n",
    "vfa_list=glob.glob(r\"*.csv\")\n",
    "vfa_list.sort()\n",
    "\n",
    "print(len(raw_list_))\n",
    "print(len(vfa_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_path=disk_path+\"\\\\CPT_TOT_DS_Stage_3_2nd\\\\VFA\\\\abet raw betch\\\\edit_raw_file\\\\\" \n",
    "raw_list_short_=[fileName for fileName in os.listdir(raw_path) if fileName.endswith(\".csv\")]\n",
    "raw_list_short=[i[:16] for i in raw_list_short_]\n",
    "vfa_list_short=[i[:16] for i in vfa_list]\n",
    "different = []\n",
    "for element in raw_list_short:\n",
    "    if element not in vfa_list_short:\n",
    "        different.append(element)\n",
    "\n",
    "different"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ True])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#check file name match\n",
    "raw_list=glob.glob(disk_path+\"\\\\CPT_TOT_DS_Stage_3_2nd\\\\VFA\\\\abet raw betch\\\\edit_raw_file\\\\*.csv\")\n",
    "raw_list.sort()\n",
    "\n",
    "path=disk_path+\"\\\\CPT_TOT_DS_Stage_3_2nd\\\\VFA\\\\dlc_output\\\\results\\\\threshold=3\\\\temp\"\n",
    "\n",
    "os.chdir(path)\n",
    "vfa_list=glob.glob(r\"*.csv\")\n",
    "vfa_list.sort()\n",
    "\n",
    "d={'raw_list':raw_list, 'vfa_list': vfa_list}\n",
    "file_list=pd.DataFrame(data=d)\n",
    "\n",
    "dosage_cond=[(file_list[\"vfa_list\"].str.contains(\"AMPH-vehicle\")),\n",
    "        (file_list[\"vfa_list\"].str.contains(\"AMPH-0.3mpk\")),\n",
    "            (file_list[\"vfa_list\"].str.contains(\"AMPH-0.6mpk\")),\n",
    "            (file_list[\"vfa_list\"].str.contains(\"AMPH-1mpk\")),]\n",
    "dosage_list=['AMPH-vehicle','AMPH-0.3mpk', 'AMPH-0.6mpk', 'AMPH-1mpk']  \n",
    "file_list['dosage']=np.select(dosage_cond, dosage_list)\n",
    "file_list[\"dt_id\"]=file_list[\"vfa_list\"]\n",
    "for h in range(len(file_list)):\n",
    "    file_list.loc[:, \"dt_id\"][h]=file_list.loc[:,'dt_id'][h].replace(\"-\" + file_list.loc[:,'dosage'][h], \"\")\n",
    "    \n",
    "\n",
    "file_list['match']=file_list.apply(lambda x : x.dt_id[:-4] in x.raw_list, axis=1)\n",
    "file_list['match'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>raw_list</th>\n",
       "      <th>vfa_list</th>\n",
       "      <th>dosage</th>\n",
       "      <th>dt_id</th>\n",
       "      <th>match</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>C:\\Users\\ye.li\\OneDrive - Lieber Institute for...</td>\n",
       "      <td>03-29-2022-A-L-2-AMPH-1mpk.csv</td>\n",
       "      <td>AMPH-1mpk</td>\n",
       "      <td>03-29-2022-A-L-2.csv</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>C:\\Users\\ye.li\\OneDrive - Lieber Institute for...</td>\n",
       "      <td>03-29-2022-A-R-1-AMPH-vehicle.csv</td>\n",
       "      <td>AMPH-vehicle</td>\n",
       "      <td>03-29-2022-A-R-1.csv</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>C:\\Users\\ye.li\\OneDrive - Lieber Institute for...</td>\n",
       "      <td>03-29-2022-B-L-7-AMPH-vehicle.csv</td>\n",
       "      <td>AMPH-vehicle</td>\n",
       "      <td>03-29-2022-B-L-7.csv</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>C:\\Users\\ye.li\\OneDrive - Lieber Institute for...</td>\n",
       "      <td>03-29-2022-B-R-8-AMPH-1mpk.csv</td>\n",
       "      <td>AMPH-1mpk</td>\n",
       "      <td>03-29-2022-B-R-8.csv</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>C:\\Users\\ye.li\\OneDrive - Lieber Institute for...</td>\n",
       "      <td>03-29-2022-B-RL-6-AMPH-0.6mpk.csv</td>\n",
       "      <td>AMPH-0.6mpk</td>\n",
       "      <td>03-29-2022-B-RL-6.csv</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>C:\\Users\\ye.li\\OneDrive - Lieber Institute for...</td>\n",
       "      <td>04-14-2022-Z-R-16-AMPH-0.6mpk.csv</td>\n",
       "      <td>AMPH-0.6mpk</td>\n",
       "      <td>04-14-2022-Z-R-16.csv</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>C:\\Users\\ye.li\\OneDrive - Lieber Institute for...</td>\n",
       "      <td>04-19-2022-W-L-2-AMPH-vehicle.csv</td>\n",
       "      <td>AMPH-vehicle</td>\n",
       "      <td>04-19-2022-W-L-2.csv</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>C:\\Users\\ye.li\\OneDrive - Lieber Institute for...</td>\n",
       "      <td>04-19-2022-Z-R-16-AMPH-0.3mpk.csv</td>\n",
       "      <td>AMPH-0.3mpk</td>\n",
       "      <td>04-19-2022-Z-R-16.csv</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>C:\\Users\\ye.li\\OneDrive - Lieber Institute for...</td>\n",
       "      <td>04-21-2022-W-L-2-AMPH-1mpk.csv</td>\n",
       "      <td>AMPH-1mpk</td>\n",
       "      <td>04-21-2022-W-L-2.csv</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>C:\\Users\\ye.li\\OneDrive - Lieber Institute for...</td>\n",
       "      <td>04-21-2022-Z-R-16-AMPH-vehicle.csv</td>\n",
       "      <td>AMPH-vehicle</td>\n",
       "      <td>04-21-2022-Z-R-16.csv</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>83 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             raw_list  \\\n",
       "0   C:\\Users\\ye.li\\OneDrive - Lieber Institute for...   \n",
       "1   C:\\Users\\ye.li\\OneDrive - Lieber Institute for...   \n",
       "2   C:\\Users\\ye.li\\OneDrive - Lieber Institute for...   \n",
       "3   C:\\Users\\ye.li\\OneDrive - Lieber Institute for...   \n",
       "4   C:\\Users\\ye.li\\OneDrive - Lieber Institute for...   \n",
       "..                                                ...   \n",
       "78  C:\\Users\\ye.li\\OneDrive - Lieber Institute for...   \n",
       "79  C:\\Users\\ye.li\\OneDrive - Lieber Institute for...   \n",
       "80  C:\\Users\\ye.li\\OneDrive - Lieber Institute for...   \n",
       "81  C:\\Users\\ye.li\\OneDrive - Lieber Institute for...   \n",
       "82  C:\\Users\\ye.li\\OneDrive - Lieber Institute for...   \n",
       "\n",
       "                              vfa_list        dosage                  dt_id  \\\n",
       "0       03-29-2022-A-L-2-AMPH-1mpk.csv     AMPH-1mpk   03-29-2022-A-L-2.csv   \n",
       "1    03-29-2022-A-R-1-AMPH-vehicle.csv  AMPH-vehicle   03-29-2022-A-R-1.csv   \n",
       "2    03-29-2022-B-L-7-AMPH-vehicle.csv  AMPH-vehicle   03-29-2022-B-L-7.csv   \n",
       "3       03-29-2022-B-R-8-AMPH-1mpk.csv     AMPH-1mpk   03-29-2022-B-R-8.csv   \n",
       "4    03-29-2022-B-RL-6-AMPH-0.6mpk.csv   AMPH-0.6mpk  03-29-2022-B-RL-6.csv   \n",
       "..                                 ...           ...                    ...   \n",
       "78   04-14-2022-Z-R-16-AMPH-0.6mpk.csv   AMPH-0.6mpk  04-14-2022-Z-R-16.csv   \n",
       "79   04-19-2022-W-L-2-AMPH-vehicle.csv  AMPH-vehicle   04-19-2022-W-L-2.csv   \n",
       "80   04-19-2022-Z-R-16-AMPH-0.3mpk.csv   AMPH-0.3mpk  04-19-2022-Z-R-16.csv   \n",
       "81      04-21-2022-W-L-2-AMPH-1mpk.csv     AMPH-1mpk   04-21-2022-W-L-2.csv   \n",
       "82  04-21-2022-Z-R-16-AMPH-vehicle.csv  AMPH-vehicle  04-21-2022-Z-R-16.csv   \n",
       "\n",
       "    match  \n",
       "0    True  \n",
       "1    True  \n",
       "2    True  \n",
       "3    True  \n",
       "4    True  \n",
       "..    ...  \n",
       "78   True  \n",
       "79   True  \n",
       "80   True  \n",
       "81   True  \n",
       "82   True  \n",
       "\n",
       "[83 rows x 5 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### sync abet raw data to vfa result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "combine_1/evnet is the last frame: 04-05-2022-Z-L-15-AMPH-1mpk.csv\n",
      "combine_1/evnet is the last frame: 04-05-2022-Z-L-15-AMPH-1mpk.csv\n",
      "combine_1/evnet is the last frame: 04-05-2022-Z-L-15-AMPH-1mpk.csv\n"
     ]
    }
   ],
   "source": [
    "# merge cpt raw data and vfa data to sync event\n",
    "#save processed file into the pathway below:\n",
    "\n",
    "pathfile=disk_path+\"\\\\CPT_TOT_DS_Stage_3_2nd\\\\VFA\\\\dlc_output\\\\results\\\\threshold=3\\\\temp_2_V2\"\n",
    "if os.path.isdir(pathfile) == False:\n",
    "    os.mkdir(pathfile)\n",
    "\n",
    "target=disk_path+\"\\\\CPT_TOT_DS_Stage_3_2nd\\\\VFA\\\\dlc_output\\\\results\\\\threshold=3\\\\temp\"\n",
    "os.chdir(target)\n",
    "\n",
    "for i, j in zip(raw_list, vfa_list):\n",
    "    df=pd.read_csv(i, index_col=None, header=0)\n",
    "    evnt=df[[\"event\", \"sec_cal\"]].copy()\n",
    "    evnt.dropna(subset=['sec_cal'], inplace=True)\n",
    "    evnt[\"count\"]=1\n",
    "    #load vfa data and reform the format of the dataframe    \n",
    "    df=pd.read_csv(j)\n",
    "\n",
    "    #seperating handling Center Screen Touch - Time event, since it comes with Hit, \n",
    "    #and makes the orientated hit tobe undeterminded\n",
    "   \n",
    "    #merge cpt raw data, not includes center screen touch event and Stim Onset - End ITI, and vfa data\n",
    "    combine_1=pd.concat([evnt[(evnt[\"event\"]!=\"Center Screen Touch - Time\") & (evnt[\"event\"]!=\"Stim Onset - End ITI\")], df], \n",
    "                        ignore_index=False).sort_values(by='sec_cal').reset_index()\n",
    "\n",
    "    #check each row, add ori_before and ori_after\n",
    "    combine_1['ori_before']=\"\"\n",
    "    combine_1[\"ori_after\"]=\"\"\n",
    "\n",
    "    for h in range(len(combine_1)):# the code in combine_1, combine_2, combine_3 are updated to fix the chained index issue\n",
    "        if (combine_1.loc[h, \"count\"]==1 and h<len(combine_1)-1):\n",
    "            combine_1.loc[h, \"ori_before\"]=combine_1.loc[h-1,'blind_or_not']\n",
    "            combine_1.loc[h, \"ori_after\"]=combine_1.loc[h+1, 'blind_or_not']\n",
    "            combine_1['ori'] = np.where(combine_1[\"ori_before\"] == combine_1[\"ori_after\"], combine_1['ori_after'], \"undetermined\")\n",
    "\n",
    "        elif (combine_1.loc[h, \"count\"]==1 and h==len(combine_1)-1):\n",
    "            print(\"combine_1/evnet is the last frame: \"+j)   \n",
    "            combine_1.loc[h, \"ori_before\"]=combine_1.loc[h-1,'blind_or_not']\n",
    "            combine_1.loc[h, 'ori'] = combine_1.loc[h, \"ori_before\"]\n",
    "\n",
    "    #merge cpt raw data only includes center screen touch event and vfa data\n",
    "    combine_2=pd.concat([evnt[evnt[\"event\"]==\"Center Screen Touch - Time\"], df], ignore_index=False).sort_values(by='sec_cal').reset_index()\n",
    "\n",
    "    #check each row, add ori_before and ori_after\n",
    "    combine_2['ori_before']=\"\"\n",
    "    combine_2[\"ori_after\"]=\"\"\n",
    "\n",
    "    for h in range(len(combine_2)):\n",
    "        if (combine_2.loc[h, \"count\"]==1 and h<len(combine_2)-1):\n",
    "            combine_2.loc[h, \"ori_before\"]=combine_2.loc[h-1,'blind_or_not']\n",
    "            combine_2.loc[h, \"ori_after\"]=combine_2.loc[h+1, 'blind_or_not']\n",
    "            combine_2['ori'] = np.where(combine_2[\"ori_before\"] == combine_2[\"ori_after\"], combine_2['ori_after'], \"undetermined\")\n",
    "\n",
    "        elif (combine_2.loc[h, \"count\"]==1 and h==len(combine_2)-1):\n",
    "            print(\"combine_1/evnet is the last frame: \"+j) \n",
    "            combine_2.loc[h, \"ori_before\"]=combine_2.loc[h-1,'blind_or_not']\n",
    "            combine_2.loc[h, 'ori'] = combine_2.loc[h, \"ori_before\"]\n",
    "\n",
    "    #merge cpt raw data only Stim Onset - End ITI and vfa data\n",
    "    combine_3=pd.concat([evnt[evnt[\"event\"]==\"Stim Onset - End ITI\"], df], ignore_index=False).sort_values(by='sec_cal').reset_index()\n",
    "\n",
    "    #check each row, add ori_before and ori_after\n",
    "    combine_3['ori_before']=\"\"\n",
    "    combine_3[\"ori_after\"]=\"\"\n",
    "\n",
    "    for h in range(len(combine_3)):\n",
    "        if (combine_3.loc[h, \"count\"]==1 and h<len(combine_3)-1):\n",
    "            combine_3.loc[h, \"ori_before\"]=combine_3.loc[h-1,'blind_or_not']\n",
    "            combine_3.loc[h, \"ori_after\"]=combine_3.loc[h+1, 'blind_or_not']\n",
    "            combine_3['ori'] = np.where(combine_3[\"ori_before\"] == combine_3[\"ori_after\"], combine_3['ori_after'], \"undetermined\")   \n",
    "\n",
    "        elif (combine_3.loc[h, \"count\"]==1 and h==len(combine_3)-1):\n",
    "            print(\"combine_1/evnet is the last frame: \"+j) \n",
    "            combine_3.loc[h, \"ori_before\"]=combine_3.loc[h-1,'blind_or_not']\n",
    "            combine_3.loc[h, 'ori'] = combine_3.loc[h, \"ori_before\"]\n",
    "\n",
    "    #combine the two part results\n",
    "    combine=pd.concat([combine_1, combine_2[combine_2[\"event\"]==\"Center Screen Touch - Time\"], \n",
    "                       combine_3[combine_3[\"event\"]==\"Stim Onset - End ITI\"]], ignore_index=False).sort_values(by='sec_cal')\n",
    "\n",
    "    combine=combine.drop(['level_0'], axis=1)\n",
    "    \n",
    "    #add 45min bin and 15min bin feature\n",
    "\n",
    "    conditions_15=[(combine[\"sec_cal\"]<0.000),\n",
    "            (combine[\"sec_cal\"]>=0.000) & (combine[\"sec_cal\"]<=900.000),\n",
    "            (combine[\"sec_cal\"]>900.000) & (combine[\"sec_cal\"]<=1800.000), \n",
    "            (combine[\"sec_cal\"]>1800.000) & (combine[\"sec_cal\"]<=2700.000),\n",
    "            (combine[\"sec_cal\"]>2700.000) & (combine[\"sec_cal\"]<=3600.000),\n",
    "            (combine[\"sec_cal\"]>3600.000) & (combine[\"sec_cal\"]<=4500.000),\n",
    "            (combine[\"sec_cal\"]>4500.000) & (combine[\"sec_cal\"]<=5400.000),\n",
    "                  (combine[\"sec_cal\"]>5400.000)]\n",
    "\n",
    "    values_15=['before_session','15bin_1', '15bin_2', '15bin_3', '15bin_4', '15bin_5', '15bin_6', 'after_session'] \n",
    "\n",
    "    conditions_45=[(combine[\"sec_cal\"]<0.000),\n",
    "                   (combine[\"sec_cal\"]>=0.000) & (combine[\"sec_cal\"]<=2700.000),\n",
    "            (combine[\"sec_cal\"]>2700.000) & (combine[\"sec_cal\"]<=5400.000), (combine[\"sec_cal\"]>5400.000)]\n",
    "\n",
    "    values_45=['before_session',\"first_45min\", \"last_45min\", \"after_session\"] \n",
    "    \n",
    "       \n",
    "    combine['45min_bin']=np.select(conditions_45, values_45)\n",
    "    \n",
    "    combine['15min_bin']=np.select(conditions_15, values_15)\n",
    "    \n",
    "    combine=combine.round(3)\n",
    "    combine['cohort']='coh2'\n",
    "    \n",
    "    combine.to_csv(pathfile + \"\\\\%s\"%j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
